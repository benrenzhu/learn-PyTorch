{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 开始"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 张量/Tensors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "张量类似于 NumPy 的 ndarrays，此外，张量还可以用于 GPU 来加速计算。  \n",
    "注：张量/tensor 可以简化理解为（多维）数组"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function #为了可以使用后续版本的特性\n",
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. 构造一个未初始化的 5*3 矩阵"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.0000e+00, -3.6893e+19,  1.3936e+23],\n",
      "        [ 2.8601e-42,  1.8361e+25,  1.4603e-19],\n",
      "        [ 6.4069e+02,  2.7489e+20,  1.5444e+25],\n",
      "        [ 1.6217e-19,  7.0062e+22,  1.6795e+08],\n",
      "        [ 4.7423e+30,  4.7393e+30,  9.5461e-01]])\n"
     ]
    }
   ],
   "source": [
    "x = torch.empty(5, 3) #空矩阵不是零矩阵，所以输出不一定是全 0\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. 构造一个随机初始化的 5*3 矩阵"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.9173, 0.7287, 0.5771],\n",
      "        [0.5158, 0.0243, 0.8110],\n",
      "        [0.5149, 0.4671, 0.0970],\n",
      "        [0.6095, 0.9521, 0.1546],\n",
      "        [0.7329, 0.1857, 0.3178]])\n"
     ]
    }
   ],
   "source": [
    "x = torch.rand(5, 3)\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. 构造一个数据类型为 long 的 5*3 矩阵"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0, 0, 0],\n",
      "        [0, 0, 0],\n",
      "        [0, 0, 0],\n",
      "        [0, 0, 0],\n",
      "        [0, 0, 0]])\n"
     ]
    }
   ],
   "source": [
    "x = torch.zeros(5, 3, dtype = torch.long)\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4. 直接用数据构造张量（tensor）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([5.5000, 3.0000])\n"
     ]
    }
   ],
   "source": [
    "x = torch.tensor([5.5, 3])\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5. 创建一个基于现有张量的张量，以下方法会重新使用原 tensor 的属性，如：dtype，除非用户提供新的数据（给这个新张量）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 1., 1.],\n",
      "        [1., 1., 1.],\n",
      "        [1., 1., 1.],\n",
      "        [1., 1., 1.],\n",
      "        [1., 1., 1.]], dtype=torch.float64)\n",
      "tensor([[ 0.2673,  0.9480, -1.9666],\n",
      "        [-0.6954, -1.6245, -0.1614],\n",
      "        [ 0.1719,  1.4551, -0.8314],\n",
      "        [-1.8049,  1.2248, -1.1919],\n",
      "        [-0.2155,  0.6382, -0.8265]])\n"
     ]
    }
   ],
   "source": [
    "x = x.new_ones(5, 3, dtype = torch.double)\n",
    "print(x)\n",
    "\n",
    "x = torch.randn_like(x, dtype = torch.float)\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6. 获取张量的尺寸"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([5, 3])\n"
     ]
    }
   ],
   "source": [
    "print(x.size())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> torch.Size 是一个元组，因此支持所有元组的操作"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 操作/Operations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 增加/add"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. add：语法1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.2696,  1.0028, -1.8249],\n",
      "        [-0.4462, -1.1442,  0.6934],\n",
      "        [ 0.9728,  2.0990, -0.7343],\n",
      "        [-1.6250,  1.2412, -0.7579],\n",
      "        [ 0.0665,  1.1310, -0.4279]])\n"
     ]
    }
   ],
   "source": [
    "y = torch.rand(5, 3)\n",
    "print(x + y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. add：语法2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.2696,  1.0028, -1.8249],\n",
      "        [-0.4462, -1.1442,  0.6934],\n",
      "        [ 0.9728,  2.0990, -0.7343],\n",
      "        [-1.6250,  1.2412, -0.7579],\n",
      "        [ 0.0665,  1.1310, -0.4279]])\n"
     ]
    }
   ],
   "source": [
    "print(torch.add(x, y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. add：提供一个用于输出的张量作为参数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.2696,  1.0028, -1.8249],\n",
      "        [-0.4462, -1.1442,  0.6934],\n",
      "        [ 0.9728,  2.0990, -0.7343],\n",
      "        [-1.6250,  1.2412, -0.7579],\n",
      "        [ 0.0665,  1.1310, -0.4279]])\n"
     ]
    }
   ],
   "source": [
    "result = torch.empty(5, 3)\n",
    "torch.add(x, y, out = result)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4. add：原地操作/in-place（不使用额外的变量）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.2696,  1.0028, -1.8249],\n",
      "        [-0.4462, -1.1442,  0.6934],\n",
      "        [ 0.9728,  2.0990, -0.7343],\n",
      "        [-1.6250,  1.2412, -0.7579],\n",
      "        [ 0.0665,  1.1310, -0.4279]])\n"
     ]
    }
   ],
   "source": [
    "y.add_(x)\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> <span id='actions_'>任何**原地**改变张量的操作都带有后缀`_`，如`x.copy_(y)`，`x.t_()`，这会改变 x 原本的值。</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5. 可以使用标准的 NumPy 索引和其他的各种功能"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 0.9480, -1.6245,  1.4551,  1.2248,  0.6382])\n"
     ]
    }
   ],
   "source": [
    "print(x[:, 1])\n",
    "#输出 x 的第 1 列（从 0 开始）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 改变尺寸/resize/reshape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. 如果需要改变张量的尺寸，可以使用 `torch.view`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 4]) torch.Size([16]) torch.Size([2, 8])\n"
     ]
    }
   ],
   "source": [
    "x = torch.randn(4, 4)              #randn 随机的正态分布\n",
    "y = x.view(16)                     #非 16 会 invalid\n",
    "z = x.view(-1, 8)\n",
    "print(x.size(), y.size(), z.size())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. 如果你有一个 1 元素的张量，通过 `.item()` 可以获取到元素的值"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.0434])\n",
      "0.04343915358185768\n"
     ]
    }
   ],
   "source": [
    "x = torch.randn(1)\n",
    "print(x)\n",
    "print(x.item())    #只有一个元素的张量才可以用.item，返回值是一个 number"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 更多 tensor 的操作，包括置换、索引、切片、数学运算、线性代数、随机数等等，可以参考[文档](https://pytorch.org/docs/torch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 与 NumPy 的互动"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Torch 的张量和 NumPy 的数组将共享它们的内存地址(如果 Torch 张量存在于 CPU 上) ，更改其中一个将另一个也会改变。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 将 Torch 张量转换为 NumPy 数组：Tensor → Array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1., 1., 1., 1., 1.])\n",
      "[1. 1. 1. 1. 1.]\n"
     ]
    }
   ],
   "source": [
    "a = torch.ones(5)\n",
    "b = a.numpy()\n",
    "print(a)\n",
    "print(b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "看看 numpy 数组的值是如何改变的"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([2., 2., 2., 2., 2.])\n",
      "[2. 2. 2. 2. 2.]\n"
     ]
    }
   ],
   "source": [
    "a.add_(1) \n",
    "print(a)\n",
    "print(b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> `add_` 为什么有 `_` 参考[这里](#actions_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 将 NumPy 数组转换为 Torch 张量"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2. 2. 2. 2. 2.]\n",
      "tensor([2., 2., 2., 2., 2.], dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "a = np.ones(5)\n",
    "b = torch.from_numpy(a)\n",
    "np.add(a, 1, out = a)    #如果 out = b 会报错：返回值必须是（numpy 的）ArrayType\n",
    "print(a)\n",
    "print(b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "所有 CPU 上的张量，除了 CharTensor，都支持与 NumPy 互相转换。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CUDA 张量"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "张量可通过 `.to` 转移到任何设备上"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 仅在有 CUDA 设备时可运行\n",
    "# 通过 torch.device 对象将张量放入/移出 GPU\n",
    "x = torch.randn(5, 3)\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")          # 一个 CUDA 设备的对象\n",
    "    y = torch.ones_like(x, device=device)  # 直接在 GPU 上新建一个张量\n",
    "    x = x.to(device)                       # 或者通过'to(\"cuda\")'转移到 GPU 上\n",
    "    z = x + y\n",
    "    print(z)\n",
    "    print(z.to(\"cpu\", torch.double))       # '.to' 可以同时修改 dtype!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  # AUTOGRAD：自动微分/AUTOMATIC DIFFERENTIATION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PyTorch 的核心就是 `autograd` 包，它可以为张量的所有操作自动求微分。他是一个“由运行定义”的框架——你可以用代码指定如何反向传播，每一次迭代都可以不一样。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 张量/Tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`torch.Tensor` 是这个包的最重要的类。如果你把张量的 `.requires_grad` 属性置为 `True`，它会追踪张量上的所有操作。结束计算时，调用 `.backward()` 可以自动计算当前张量的所有梯度并自动累加在 `.grad` 属性上。  \n",
    "  \n",
    "若要停止追踪一个张量，你可以调用 `.detach()` 来把当前张量从计算历史中剥离，这样后续的张量计算就不会被追踪。  \n",
    "  \n",
    "为了避免继续追踪张量的计算以及内存占用，你可以用 `with torch.no_grad():` 来包裹代码块。这样操作在评估模型时很有用，因为模型在 `requires)grad=True` 时，参数可以被训练；但是评估模型时并不需要梯度（来训练模型）。  \n",
    "  \n",
    "还有一个在自动计算梯度是很重要的类：`Function`  \n",
    "  \n",
    "`Tensor` 和 `Function` 相互配合建立起一个无环图，这个无环图包含了所有计算的历史记录。每个张量都有一个 `.grad_fn` 属性，这个属性是创建这个 `Tensor` 的 `Function` 类的引用。（除了那些用户设定 `grad_fn is None` 的张量）  \n",
    "  \n",
    "如果你要计算导数，你可以调用 `Tensor` 的 `.backward()`。如果 `Tensor` 是一个标量（比如只有一个元素的张量），不需要为 `backward()` 指定任何参数，一旦这个张量有不止一个元素， 你就需要指定一个跟张量维度匹配的 `gradient` 参数。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 创建一个张量，并设置 `requires_grad=True` 来追踪计算"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 1.],\n",
      "        [1., 1.]], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "x = torch.ones(2, 2, requires_grad = True)\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 操作一次张量"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[3., 3.],\n",
      "        [3., 3.]], grad_fn=<AddBackward0>)\n"
     ]
    }
   ],
   "source": [
    "y = x + 2\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "y 作为一次张量操作的结果，具有 `grad_fn` 属性"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<AddBackward0 object at 0x12fd46978>\n"
     ]
    }
   ],
   "source": [
    "print(y.grad_fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 在 y 上执行更多操作"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[27., 27.],\n",
      "        [27., 27.]], grad_fn=<MulBackward0>) tensor(27., grad_fn=<MeanBackward0>)\n"
     ]
    }
   ],
   "source": [
    "z = y * y * 3\n",
    "out = z.mean()\n",
    "print(z, out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`requires_grad_()` 可以原地(in-place)修改现有张量的 `requires_grad`，默认参数为 `False`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n",
      "True\n",
      "True\n",
      "<SumBackward0 object at 0x12fd5fba8>\n"
     ]
    }
   ],
   "source": [
    "a = torch.randn(2, 2)\n",
    "a = ((a * 3) / (a -1))\n",
    "print(a.requires_grad)\n",
    "\n",
    "a.requires_grad_(True)\n",
    "print(a.requires_grad)\n",
    "\n",
    "b = (a * a).sum()\n",
    "print(b.requires_grad)\n",
    "print(b.grad_fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 梯度/Gradients"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我们现在开始反向传播，因为 `out` 包含了一个标量，所以 `out.backward()` 等价于 `out.backward(torch.tensor(1.))`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "out.backward()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 打印出 x 的微分"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[4.5000, 4.5000],\n",
      "        [4.5000, 4.5000]])\n"
     ]
    }
   ],
   "source": [
    "print(x.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "你会得到一个都是 `4.5` 的矩阵。我们把张量 `out` 称作 “*o*”，则有 $o = \\frac{1}{4}\\sum_i z_i$，$z_i = 3(x_i+2)^2$，$z_i\\bigr\\rvert_{x_i=1} = 27$  \n",
    "因此 $o$ 对 $x$ 的微分为 $\\frac{\\partial o}{\\partial x_i} = \\frac{3}{2}(x_i+2)$，所以$\\frac{\\partial o}{\\partial x_i}\\bigr\\rvert_{x_i=1} = \\frac{9}{2} = 4.5$  \n",
    "从数学角度讲，如果有一个向量 $\\vec{y}$，$\\vec{y}=f(\\vec{x})$，$\\vec{y}$ 关于 $\\vec{x}$ 的梯度是一个雅各比矩阵：  \n",
    "\n",
    "$$\n",
    "\\begin{split}J=\\left(\\begin{array}{ccc}\n",
    " \\frac{\\partial y_{1}}{\\partial x_{1}} & \\cdots & \\frac{\\partial y_{1}}{\\partial x_{n}}\\\\\n",
    " \\vdots & \\ddots & \\vdots\\\\\n",
    " \\frac{\\partial y_{m}}{\\partial x_{1}} & \\cdots & \\frac{\\partial y_{m}}{\\partial x_{n}}\n",
    " \\end{array}\\right)\\end{split}\n",
    "$$\n",
    " \n",
    " 一般来讲，`torch.autograd` 是一个计算向量-雅各比乘积的引擎。即，给定一个向量 $v=\\left(\\begin{array}{cccc} v_{1} & v_{2} & \\cdots & v_{m}\\end{array}\\right)^{T}$，计算乘积 $v^{T}\\cdot J$。  \n",
    " 如果 $v$ 恰好是标量函数 $l=g\\left(\\vec{y}\\right)$的梯度，即 $v=\\left(\\begin{array}{ccc}\\frac{\\partial l}{\\partial y_{1}} & \\cdots & \\frac{\\partial l}{\\partial y_{m}}\\end{array}\\right)^{T}$，则根据链式法则，向量-雅各比的乘积就是向量 $l$ 对 $\\vec{x}$ 的梯度：  \n",
    " \n",
    "$$\n",
    "\\begin{split}J^{T}\\cdot v=\\left(\\begin{array}{ccc}\n",
    " \\frac{\\partial y_{1}}{\\partial x_{1}} & \\cdots & \\frac{\\partial y_{m}}{\\partial x_{1}}\\\\\n",
    " \\vdots & \\ddots & \\vdots\\\\\n",
    " \\frac{\\partial y_{1}}{\\partial x_{n}} & \\cdots & \\frac{\\partial y_{m}}{\\partial x_{n}}\n",
    " \\end{array}\\right)\\left(\\begin{array}{c}\n",
    " \\frac{\\partial l}{\\partial y_{1}}\\\\\n",
    " \\vdots\\\\\n",
    " \\frac{\\partial l}{\\partial y_{m}}\n",
    " \\end{array}\\right)=\\left(\\begin{array}{c}\n",
    " \\frac{\\partial l}{\\partial x_{1}}\\\\\n",
    " \\vdots\\\\\n",
    " \\frac{\\partial l}{\\partial x_{n}}\n",
    " \\end{array}\\right)\\end{split}\n",
    "$$  \n",
    " \n",
    "（注意，$v^{T}\\cdot J$ 返回的是行向量，也可以通过 $J^{T}\\cdot v$ 来得到列向量）  \n",
    "  \n",
    "向量-雅可比乘积的这一特性使得向具有非标量输出的模型输入外部梯度变得非常方便。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "下面来看一个向量-雅各比乘积的例子："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1541.6862, -273.6324, -344.0537], grad_fn=<MulBackward0>)\n"
     ]
    }
   ],
   "source": [
    "x = torch.randn(3, requires_grad=True)\n",
    "\n",
    "y = x * 2\n",
    "while y.data.norm() < 1000:\n",
    "    y = y * 2\n",
    "    \n",
    "print(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在这个例子中，`y` 不再是标量。`torch.autograd` 不能直接计算完整的雅各比矩阵，但如果我们想得到向量-雅各比乘积，只需要把向量作为参数传递给 `backward` 即可："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([2.0480e+02, 2.0480e+03, 2.0480e-01])\n"
     ]
    }
   ],
   "source": [
    "v = torch.tensor([0.1, 1.0, 0.0001], dtype = torch.float)\n",
    "y.backward(v)\n",
    "\n",
    "print(x.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "想停止追踪张量的计算历史，你可以把张量的 `.requires_grad` 设为 `True` ，也可以把代码块包在 `with torch.no_grad():` 中："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "True\n",
      "False\n"
     ]
    }
   ],
   "source": [
    "print(x.requires_grad)\n",
    "print((x ** 2).requires_grad)\n",
    "\n",
    "with torch.no_grad():\n",
    "    print((x ** 2).requires_grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "或是使用 `.detach()` 来获得一个内容相同但不需要梯度的新张量："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "False\n",
      "tensor(True)\n"
     ]
    }
   ],
   "source": [
    "print(x.requires_grad)\n",
    "y = x.detach()\n",
    "print(y.requires_grad)\n",
    "print(x.eq(y).all()) #不加.all()会返回一个跟 x 的 shape 相同的张量（即每个元素都进行对比）"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
