{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 一、开始"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. 张量/Tensors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "张量类似于 NumPy 的 ndarrays，此外，张量还可以用于 GPU 来加速计算。  \n",
    "注：张量/tensor 可以简化理解为（多维）数组"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function #为了可以使用后续版本的特性\n",
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. 构造一个未初始化的 5*3 矩阵"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-9.7446e-34,  7.0205e-43, -9.7446e-34],\n",
      "        [ 7.0205e-43, -9.7422e-34,  7.0205e-43],\n",
      "        [-9.7422e-34,  7.0205e-43, -9.7433e-34],\n",
      "        [ 7.0205e-43, -9.7433e-34,  7.0205e-43],\n",
      "        [-9.7422e-34,  7.0205e-43, -9.7422e-34]])\n"
     ]
    }
   ],
   "source": [
    "x = torch.empty(5, 3) #空矩阵不是零矩阵，所以输出不一定是全 0\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. 构造一个随机初始化的 5*3 矩阵"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.8628, 0.7494, 0.5244],\n",
      "        [0.0459, 0.0778, 0.1850],\n",
      "        [0.2681, 0.6595, 0.1488],\n",
      "        [0.0242, 0.1229, 0.5658],\n",
      "        [0.6376, 0.9615, 0.6427]])\n"
     ]
    }
   ],
   "source": [
    "x = torch.rand(5, 3)\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. 构造一个数据类型为 long 的 5*3 矩阵"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0, 0, 0],\n",
      "        [0, 0, 0],\n",
      "        [0, 0, 0],\n",
      "        [0, 0, 0],\n",
      "        [0, 0, 0]])\n"
     ]
    }
   ],
   "source": [
    "x = torch.zeros(5, 3, dtype = torch.long)\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4. 直接用数据构造张量（tensor）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([5.5000, 3.0000])\n"
     ]
    }
   ],
   "source": [
    "x = torch.tensor([5.5, 3])\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5. 创建一个基于现有张量的张量，以下方法会重新使用原 tensor 的属性，如：dtype，除非用户提供新的数据（给这个新张量）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 1., 1.],\n",
      "        [1., 1., 1.],\n",
      "        [1., 1., 1.],\n",
      "        [1., 1., 1.],\n",
      "        [1., 1., 1.]], dtype=torch.float64)\n",
      "tensor([[-0.6686,  0.1970,  1.8154],\n",
      "        [ 1.2458, -0.4182,  1.3414],\n",
      "        [-1.7996,  0.6116,  0.5674],\n",
      "        [ 1.1152, -0.3058,  0.1233],\n",
      "        [-0.4813, -0.1127, -0.6067]])\n"
     ]
    }
   ],
   "source": [
    "x = x.new_ones(5, 3, dtype = torch.double)\n",
    "print(x)\n",
    "\n",
    "x = torch.randn_like(x, dtype = torch.float)\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6. 获取张量的尺寸"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([5, 3])\n"
     ]
    }
   ],
   "source": [
    "print(x.size())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> torch.Size 是一个元组，因此支持所有元组的操作"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. 操作/Operations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 增加/add"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. add：语法1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.0393,  1.1786,  2.4042],\n",
      "        [ 1.3419, -0.2450,  1.8832],\n",
      "        [-1.6965,  0.9630,  0.6316],\n",
      "        [ 1.4473, -0.0189,  0.9619],\n",
      "        [-0.2346,  0.6974, -0.5788]])\n"
     ]
    }
   ],
   "source": [
    "y = torch.rand(5, 3)\n",
    "print(x + y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. add：语法2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.0393,  1.1786,  2.4042],\n",
      "        [ 1.3419, -0.2450,  1.8832],\n",
      "        [-1.6965,  0.9630,  0.6316],\n",
      "        [ 1.4473, -0.0189,  0.9619],\n",
      "        [-0.2346,  0.6974, -0.5788]])\n"
     ]
    }
   ],
   "source": [
    "print(torch.add(x, y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. add：提供一个用于输出的张量作为参数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.0393,  1.1786,  2.4042],\n",
      "        [ 1.3419, -0.2450,  1.8832],\n",
      "        [-1.6965,  0.9630,  0.6316],\n",
      "        [ 1.4473, -0.0189,  0.9619],\n",
      "        [-0.2346,  0.6974, -0.5788]])\n"
     ]
    }
   ],
   "source": [
    "result = torch.empty(5, 3)\n",
    "torch.add(x, y, out = result)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4. add：原地操作/in-place（不使用额外的变量）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.0393,  1.1786,  2.4042],\n",
      "        [ 1.3419, -0.2450,  1.8832],\n",
      "        [-1.6965,  0.9630,  0.6316],\n",
      "        [ 1.4473, -0.0189,  0.9619],\n",
      "        [-0.2346,  0.6974, -0.5788]])\n"
     ]
    }
   ],
   "source": [
    "y.add_(x)\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> <span id='actions_'>任何**原地**改变张量的操作都带有后缀`_`，如`x.copy_(y)`，`x.t_()`，这会改变 x 原本的值。</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5. 可以使用标准的 NumPy 索引和其他的各种功能"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 0.1970, -0.4182,  0.6116, -0.3058, -0.1127])\n"
     ]
    }
   ],
   "source": [
    "print(x[:, 1])\n",
    "#输出 x 的第 1 列（从 0 开始）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 改变尺寸/resize/reshape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. 如果需要改变张量的尺寸，可以使用 `torch.view`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 4]) torch.Size([16]) torch.Size([2, 8])\n"
     ]
    }
   ],
   "source": [
    "x = torch.randn(4, 4)              #randn 随机的正态分布\n",
    "y = x.view(16)                     #非 16 会 invalid\n",
    "z = x.view(-1, 8)\n",
    "print(x.size(), y.size(), z.size())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. 如果你有一个 1 元素的张量，通过 `.item()` 可以获取到元素的值"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.5318])\n",
      "0.5318058133125305\n"
     ]
    }
   ],
   "source": [
    "x = torch.randn(1)\n",
    "print(x)\n",
    "print(x.item())    #只有一个元素的张量才可以用.item，返回值是一个 number"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 更多 tensor 的操作，包括置换、索引、切片、数学运算、线性代数、随机数等等，可以参考[文档](https://pytorch.org/docs/torch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. 与 NumPy 的互动"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Torch 的张量和 NumPy 的数组将共享它们的内存地址(如果 Torch 张量存在于 CPU 上) ，更改其中一个将另一个也会改变。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 将 Torch 张量转换为 NumPy 数组：Tensor → Array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1., 1., 1., 1., 1.])\n",
      "[1. 1. 1. 1. 1.]\n"
     ]
    }
   ],
   "source": [
    "a = torch.ones(5)\n",
    "b = a.numpy()\n",
    "print(a)\n",
    "print(b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "看看 numpy 数组的值是如何改变的"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([2., 2., 2., 2., 2.])\n",
      "[2. 2. 2. 2. 2.]\n"
     ]
    }
   ],
   "source": [
    "a.add_(1) \n",
    "print(a)\n",
    "print(b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> `add_` 为什么有 `_` 参考[这里](#actions_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 将 NumPy 数组转换为 Torch 张量"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2. 2. 2. 2. 2.]\n",
      "tensor([2., 2., 2., 2., 2.], dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "a = np.ones(5)\n",
    "b = torch.from_numpy(a)\n",
    "np.add(a, 1, out = a)    #如果 out = b 会报错：返回值必须是（numpy 的）ArrayType\n",
    "print(a)\n",
    "print(b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "所有 CPU 上的张量，除了 CharTensor，都支持与 NumPy 互相转换。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. CUDA 张量"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "张量可通过 `.to` 转移到任何设备上"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 1.2651, -0.7222,  0.1449],\n",
      "        [ 2.1100,  1.2102, -0.1384],\n",
      "        [ 0.4214,  0.8702,  1.5608],\n",
      "        [ 3.0799,  1.2712, -0.3284],\n",
      "        [ 2.5014, -0.1358,  3.0337]], device='cuda:0')\n",
      "tensor([[ 1.2651, -0.7222,  0.1449],\n",
      "        [ 2.1100,  1.2102, -0.1384],\n",
      "        [ 0.4214,  0.8702,  1.5608],\n",
      "        [ 3.0799,  1.2712, -0.3284],\n",
      "        [ 2.5014, -0.1358,  3.0337]], dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "# 仅在有 CUDA 设备时可运行\n",
    "# 通过 torch.device 对象将张量放入/移出 GPU\n",
    "x = torch.randn(5, 3)\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")          # 一个 CUDA 设备的对象\n",
    "    y = torch.ones_like(x, device=device)  # 直接在 GPU 上新建一个张量\n",
    "    x = x.to(device)                       # 或者通过'to(\"cuda\")'转移到 GPU 上\n",
    "    z = x + y\n",
    "    print(z)\n",
    "    print(z.to(\"cpu\", torch.double))       # '.to' 可以同时修改 dtype!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  # 二、AUTOGRAD：自动微分/AUTOMATIC DIFFERENTIATION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PyTorch 的核心就是 `autograd` 包，它可以为张量的所有操作自动求微分。他是一个“由运行定义”的框架——你可以用代码指定如何反向传播，每一次迭代都可以不一样。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. 张量/Tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`torch.Tensor` 是这个包的最重要的类。如果你把张量的 `.requires_grad` 属性置为 `True`，它会追踪张量上的所有操作。结束计算时，调用 `.backward()` 可以自动计算当前张量的所有梯度并自动累加在 `.grad` 属性上。  \n",
    "  \n",
    "若要停止追踪一个张量，你可以调用 `.detach()` 来把当前张量从计算历史中剥离，这样后续的张量计算就不会被追踪。  \n",
    "  \n",
    "为了避免继续追踪张量的计算以及内存占用，你可以用 `with torch.no_grad():` 来包裹代码块。这样操作在评估模型时很有用，因为模型在 `requires)grad=True` 时，参数可以被训练；但是评估模型时并不需要梯度（来训练模型）。  \n",
    "  \n",
    "还有一个在自动计算梯度是很重要的类：`Function`  \n",
    "  \n",
    "`Tensor` 和 `Function` 相互配合建立起一个无环图，这个无环图包含了所有计算的历史记录。每个张量都有一个 `.grad_fn` 属性，这个属性是创建这个 `Tensor` 的 `Function` 类的引用。（除了那些用户设定 `grad_fn is None` 的张量）  \n",
    "  \n",
    "如果你要计算导数，你可以调用 `Tensor` 的 `.backward()`。如果 `Tensor` 是一个标量（比如只有一个元素的张量），不需要为 `backward()` 指定任何参数，一旦这个张量有不止一个元素， 你就需要指定一个跟张量维度匹配的 `gradient` 参数。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 创建一个张量，并设置 `requires_grad=True` 来追踪计算"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 1.],\n",
      "        [1., 1.]], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "x = torch.ones(2, 2, requires_grad = True)\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 操作一次张量"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[3., 3.],\n",
      "        [3., 3.]], grad_fn=<AddBackward0>)\n"
     ]
    }
   ],
   "source": [
    "y = x + 2\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "y 作为一次张量操作的结果，具有 `grad_fn` 属性"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<AddBackward0 object at 0x000001F5B7ED7B70>\n"
     ]
    }
   ],
   "source": [
    "print(y.grad_fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 在 y 上执行更多操作"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[27., 27.],\n",
      "        [27., 27.]], grad_fn=<MulBackward0>) tensor(27., grad_fn=<MeanBackward0>)\n"
     ]
    }
   ],
   "source": [
    "z = y * y * 3\n",
    "out = z.mean()\n",
    "print(z, out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`requires_grad_()` 可以原地(in-place)修改现有张量的 `requires_grad`，默认参数为 `False`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n",
      "True\n",
      "True\n",
      "<SumBackward0 object at 0x000001F5B7ED0B70>\n"
     ]
    }
   ],
   "source": [
    "a = torch.randn(2, 2)\n",
    "a = ((a * 3) / (a -1))\n",
    "print(a.requires_grad)\n",
    "\n",
    "a.requires_grad_(True)\n",
    "print(a.requires_grad)\n",
    "\n",
    "b = (a * a).sum()\n",
    "print(b.requires_grad)\n",
    "print(b.grad_fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. 梯度/Gradients"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我们现在开始反向传播，因为 `out` 包含了一个标量，所以 `out.backward()` 等价于 `out.backward(torch.tensor(1.))`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "out.backward()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 打印出 x 的微分"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[4.5000, 4.5000],\n",
      "        [4.5000, 4.5000]])\n"
     ]
    }
   ],
   "source": [
    "print(x.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "你会得到一个都是 `4.5` 的矩阵。我们把张量 `out` 称作 “*o*”，则有 $o = \\frac{1}{4}\\sum_i z_i$，$z_i = 3(x_i+2)^2$，$z_i\\bigr\\rvert_{x_i=1} = 27$  \n",
    "因此 $o$ 对 $x$ 的微分为 $\\frac{\\partial o}{\\partial x_i} = \\frac{3}{2}(x_i+2)$，所以$\\frac{\\partial o}{\\partial x_i}\\bigr\\rvert_{x_i=1} = \\frac{9}{2} = 4.5$  \n",
    "从数学角度讲，如果有一个向量 $\\vec{y}$，$\\vec{y}=f(\\vec{x})$，$\\vec{y}$ 关于 $\\vec{x}$ 的梯度是一个雅各比矩阵：  \n",
    "\n",
    "$$\n",
    "\\begin{split}J=\\left(\\begin{array}{ccc}\n",
    " \\frac{\\partial y_{1}}{\\partial x_{1}} & \\cdots & \\frac{\\partial y_{1}}{\\partial x_{n}}\\\\\n",
    " \\vdots & \\ddots & \\vdots\\\\\n",
    " \\frac{\\partial y_{m}}{\\partial x_{1}} & \\cdots & \\frac{\\partial y_{m}}{\\partial x_{n}}\n",
    " \\end{array}\\right)\\end{split}\n",
    "$$\n",
    " \n",
    " 一般来说，`torch.autograd` 是一个计算向量-雅各比乘积的引擎。即，给定一个向量 $v=\\left(\\begin{array}{cccc} v_{1} & v_{2} & \\cdots & v_{m}\\end{array}\\right)^{T}$，计算乘积 $v^{T}\\cdot J$（$1\\times m$ 的向量乘 $m\\times n$ 的矩阵）。  \n",
    " 如果 $v$ 是标量函数 $l=g\\left(\\vec{y}\\right)$的梯度，即 $v=\\left(\\begin{array}{ccc}\\frac{\\partial l}{\\partial y_{1}} & \\cdots & \\frac{\\partial l}{\\partial y_{m}}\\end{array}\\right)^{T}$，则根据链式法则，向量-雅各比的乘积就是向量 $l$ 对 $\\vec{x}$ 的梯度：  \n",
    " \n",
    "$$\n",
    "\\begin{split}J^{T}\\cdot v=\\left(\\begin{array}{ccc}\n",
    " \\frac{\\partial y_{1}}{\\partial x_{1}} & \\cdots & \\frac{\\partial y_{m}}{\\partial x_{1}}\\\\\n",
    " \\vdots & \\ddots & \\vdots\\\\\n",
    " \\frac{\\partial y_{1}}{\\partial x_{n}} & \\cdots & \\frac{\\partial y_{m}}{\\partial x_{n}}\n",
    " \\end{array}\\right)\\left(\\begin{array}{c}\n",
    " \\frac{\\partial l}{\\partial y_{1}}\\\\\n",
    " \\vdots\\\\\n",
    " \\frac{\\partial l}{\\partial y_{m}}\n",
    " \\end{array}\\right)=\\left(\\begin{array}{c}\n",
    " \\frac{\\partial l}{\\partial x_{1}}\\\\\n",
    " \\vdots\\\\\n",
    " \\frac{\\partial l}{\\partial x_{n}}\n",
    " \\end{array}\\right)\\end{split}\n",
    "$$  \n",
    " \n",
    "（注意，$v^{T}\\cdot J$ 返回的是行向量，也可以通过 $J^{T}\\cdot v$ 来得到列向量）  \n",
    "  \n",
    "向量-雅可比乘积的这一特性使得向具有非标量输出的模型输入外部梯度变得非常方便。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "下面来看一个向量-雅各比乘积的例子："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 446.2126, -766.6046, -716.9565], grad_fn=<MulBackward0>)\n"
     ]
    }
   ],
   "source": [
    "x = torch.randn(3, requires_grad=True)\n",
    "\n",
    "y = x * 2\n",
    "while y.data.norm() < 1000:\n",
    "    y = y * 2\n",
    "    \n",
    "print(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在这个例子中，`y` 不再是标量。`torch.autograd` 不能直接计算完整的雅各比矩阵，但如果我们想得到向量-雅各比乘积，只需要把向量作为参数传递给 `backward` 即可："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([5.1200e+01, 5.1200e+02, 5.1200e-02])\n"
     ]
    }
   ],
   "source": [
    "v = torch.tensor([0.1, 1.0, 0.0001], dtype = torch.float) #v 相当于一个权值向量\n",
    "y.backward(v)\n",
    "\n",
    "print(x.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "想停止追踪张量的计算历史，你可以把张量的 `.requires_grad` 设为 `True` ，也可以把代码块包在 `with torch.no_grad():` 中："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "True\n",
      "False\n"
     ]
    }
   ],
   "source": [
    "print(x.requires_grad)\n",
    "print((x ** 2).requires_grad)\n",
    "\n",
    "with torch.no_grad():\n",
    "    print((x ** 2).requires_grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "或是使用 `.detach()` 来获得一个内容相同但不需要梯度的新张量："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "False\n",
      "tensor(True)\n"
     ]
    }
   ],
   "source": [
    "print(x.requires_grad)\n",
    "y = x.detach()\n",
    "print(y.requires_grad)\n",
    "print(x.eq(y).all()) #不加.all()会返回一个跟 x 的 shape 相同的张量（即每个元素都进行对比）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 三、神经网络/Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "神经网络可由 `torch.mm` 包创建。  \n",
    "\n",
    "既然你已经大致了解过 `autograd` 了，其实 `nn` 正是依赖 `autograd` 来定义和微分神经网络。  \n",
    "`nn.module` 包含了神经网络的各层，以及用来返回 `output` 的方法 `forward(input)`  \n",
    "\n",
    "举个例子，以下是一个用来分类数字图片的（卷积神经）网络：\n",
    "![convnet](https://pytorch.org/tutorials/_images/mnist.png)  \n",
    "\n",
    "这是一个简单的前反馈网络。网络将输入的数据一层一层地传递并输出最终结果。  \n",
    "\n",
    "神经网络的一个典型的训练流程如下：\n",
    "  - 定义一个具有可学习的参数或者权重的神经网络\n",
    "  - 迭代数据集作为输入\n",
    "  - 通过神经网络处理输入的数据\n",
    "  - 计算损失函数（距离正确的输出还有多远）\n",
    "  - 把损失函数的梯度传递给参数\n",
    "  - 更新网络的权重/参数，规则往往很简单：`新权重 = 旧权重 - 学习率 * 梯度`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. 定义网络/Define the network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我们来定义一个网络："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Net(\n",
      "  (conv1): Conv2d(1, 6, kernel_size=(3, 3), stride=(1, 1))\n",
      "  (conv2): Conv2d(6, 16, kernel_size=(3, 3), stride=(1, 1))\n",
      "  (fc1): Linear(in_features=576, out_features=120, bias=True)\n",
      "  (fc2): Linear(in_features=120, out_features=84, bias=True)\n",
      "  (fc3): Linear(in_features=84, out_features=10, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn \n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "class Net(nn.Module):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        #1 个输入图像通道，6 个输出通道，3*3 的方形卷积核\n",
    "        \n",
    "        self.conv1 = nn.Conv2d(1, 6, 3)\n",
    "        self.conv2 = nn.Conv2d(6, 16, 3)\n",
    "        #做一次仿射变换 y = Wx + b\n",
    "        self.fc1 = nn.Linear(16 * 6 * 6, 120) #nn.Liner 是全连接层，6*6是图像的尺寸\n",
    "        self.fc2 = nn.Linear(120, 84)\n",
    "        self.fc3 = nn.Linear(84, 10)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        #在 2*2大小的窗口上做 Max pooling\n",
    "        x = F.max_pool2d(F.relu(self.conv1(x)), (2, 2))\n",
    "        #如果 max pool 是一个正方形，你可以只输入一个值\n",
    "        x = F.max_pool2d(F.relu(self.conv2(x)), 2)\n",
    "        x = x.view(-1, self.num_flat_features(x))\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "    \n",
    "    def num_flat_features(self, x):\n",
    "        size = x.size()[1:] #除了 batch 外的所有维度\n",
    "        num_features = 1\n",
    "        for s in size:\n",
    "            num_features *= s\n",
    "        return num_features\n",
    "        \n",
    "net = Net()\n",
    "print(net)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "你只需定义 `forward` 函数，`autograd` 会自动为你定义 `backward` 函数（用来计算梯度）。你可以在 `forward` 函数里使用张量的任何操作。  \n",
    "模型的可学习参数可由 `net.parameters()` 返回。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "conv1.weight torch.Size([6, 1, 3, 3])\n",
      "conv1.bias torch.Size([6])\n",
      "conv2.weight torch.Size([16, 6, 3, 3])\n",
      "conv2.bias torch.Size([16])\n",
      "fc1.weight torch.Size([120, 576])\n",
      "fc1.bias torch.Size([120])\n",
      "fc2.weight torch.Size([84, 120])\n",
      "fc2.bias torch.Size([84])\n",
      "fc3.weight torch.Size([10, 84])\n",
      "fc3.bias torch.Size([10])\n",
      "10\n",
      "torch.Size([6, 1, 3, 3])\n"
     ]
    }
   ],
   "source": [
    "for name, param in net.named_parameters():\n",
    "    print(name, param.shape)\n",
    "\n",
    "params = list(net.parameters())\n",
    "print(len(params))\n",
    "print(params[0].size())  #conv1's .weight"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我们来尝试一个 $32\\times 32$ 随机输入的 LeNet，要在 MNIST 数据集上使用这个网络，需要先将数据集的图像尺寸转换成 $32\\times 32$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.0310, -0.0331, -0.0646,  0.0563,  0.0041,  0.0210, -0.0866,  0.0300,\n",
      "          0.0376, -0.0893]], grad_fn=<AddmmBackward>)\n"
     ]
    }
   ],
   "source": [
    "input = torch.randn(1, 1, 32, 32)\n",
    "out = net(input)\n",
    "print(out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "将梯度缓存归零并且向后传播随机的梯度："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "net.zero_grad()\n",
    "out.backward(torch.randn(1, 10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> `torch.nn` 只支持 mini-batches，整个 torch.nn 包只支持小批量样本的输入，而不支持单个样本。\n",
    "比如，`nn.Conv2d` 需要一个 4 维张量 `nSamples * nChannels * Height * Width`  \n",
    "如果只有一个样本，只需使用 `input.unsqueeze(0)` 添加一个假的批处理尺寸。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "再进一步处理前，让我们回顾一下你已经学会了什么。  \n",
    "### 回顾：\n",
    "- `torch.Tensor` 是一个支持像 `backward` 等自动微分操作的多维数组\n",
    "- `nn.Module` 神经网络模块，方便的参数封装方式，有助于将参数移动到 GPU、导出、加载等等\n",
    "- `nn.Parameter` 作为一类张量，当作为属性分配给 `Module` 时，自动注册为参数。\n",
    "- `autograd.Function` 实现自动微分操作的正向和反向传播。每个 `Tensor` 至少创建一个 `Function` 节点，该节点来连接到创建 `Tensor` 的函数并对其历史进行编码。  \n",
    "\n",
    "### 目前为止，我们进行了：  \n",
    "- 定义一个神经网络  \n",
    "- 处理输入数据和调用反向传播  \n",
    "\n",
    "### 接下来：  \n",
    "- 计算损失函数  \n",
    "- 更新网络内的权重"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. 损失函数/Loss Function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "损失函数接受输入对(输出，目标)，并估算出输出值于目标间的距离。  \n",
    "在 nn 包中有几种不同的[损失函数](https://pytorch.org/docs/nn.html#loss-functions)。一个简单的损失定义是: `nn.MSELoss`，它计算输入值和目标值之间的**均方误差**。\n",
    "\n",
    "举例如下："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(1.0628, grad_fn=<MseLossBackward>)\n"
     ]
    }
   ],
   "source": [
    "output = net(input)\n",
    "target = torch.randn(10)  # 价目表，就举个例子,这是一个一维数组\n",
    "target = target.view(1, -1)  # 跟输出的维度一致，变成[1*10]的张量\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "loss = criterion(output, target)\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "现在，如果你沿着损失的反方向，使用它的 `.grad_fn` 属性，你会看到一个如下的计算图:\n",
    "```\n",
    "input -> conv2d -> relu -> maxpool2d -> conv2d -> relu -> maxpool2d\n",
    "      -> view -> linear -> relu -> linear -> relu -> linear\n",
    "      -> MSELoss\n",
    "      -> loss\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "所以，当我们调用 `loss.backward()` ，会按照整个计算图对损失函数求微分。图中 `requires_grad=True` 的张量具有 `.grad` 属性，这个属性是一个用来累加梯度的张量。  \n",
    "\n",
    "为了说明这一点，我们来反向推几步："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<MseLossBackward object at 0x000001F5B7F23780>\n",
      "<AddmmBackward object at 0x000001F5B7F238D0>\n",
      "<AccumulateGrad object at 0x000001F5B7F23780>\n"
     ]
    }
   ],
   "source": [
    "print(loss.grad_fn)                                           #均方误差/MESLoss\n",
    "print(loss.grad_fn.next_functions[0][0])                      #全连接层/Linear\n",
    "print(loss.grad_fn.next_functions[0][0].next_functions[0][0]) #线性整流函数/ReLU（激活函数）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. 反向传播/Backprop <a id='backprop'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我们只需使用 `loss.backward()` 即可反向传播误差，你需要清除已有的梯度，否则新的梯度会被累加到旧的梯度上。  \n",
    "  \n",
    "现在我们来调用 `loss.backward()`，看看在反向传播前后 conv1 偏差/bias 的梯度"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "反向传播前的 conv1.bias.grad\n",
      "tensor([0., 0., 0., 0., 0., 0.])\n",
      "反向传播后的 conv1.bias.grad\n",
      "tensor([-0.0140,  0.0072,  0.0046,  0.0135,  0.0021, -0.0221])\n"
     ]
    }
   ],
   "source": [
    "net.zero_grad()                 #将所有参数的梯度归零\n",
    "print('反向传播前的 conv1.bias.grad')\n",
    "print(net.conv1.bias.grad)\n",
    "\n",
    "loss.backward()\n",
    "\n",
    "print('反向传播后的 conv1.bias.grad')\n",
    "print(net.conv1.bias.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "好，你已经知道了如何使用损失函数了。  \n",
    "\n",
    "### 稍后阅读：  \n",
    "- 神经网络包 `nn` 囊括了构成深层神经网络的各种模块和损失函数。完整的文档列表在[这里](https://pytorch.org/docs/nn)。  \n",
    "\n",
    "### 只剩下一个没学：\n",
    "- 更新网络的权值"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. 更新网络的权值/Update the weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "实践中最常用的就是随机梯度下降法（SGD）：\n",
    "`权重 = 权重 - 学习率 * 梯度`  \n",
    "\n",
    "我们可以用下面很简单的 Python 代码实现它："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.01\n",
    "for f in net.parameters():\n",
    "    f.data.sub_(f.grad.data * learning_rate)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "然而，在你使用神经网络时，你想使用各种不同的更新算法，比如SGD、Nesterov-SGD、Adam、RMSProp等等。为了实现这点，我们构建了一个包：`torch.optim`，实现了所有上述的算法。用起来很简单："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "#创建你的优化器\n",
    "optimizer = optim.SGD(net.parameters(), lr = 0.1)\n",
    "\n",
    "#在你的训练循环中\n",
    "optimizer.zero_grad()              #将梯度的缓存归零\n",
    "output = net(input)\n",
    "loss = criterion(output, target)\n",
    "loss.backward()\n",
    "optimizer.step() #执行更新操作"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> 观察梯度缓冲区是如何使用 `optimizer.zero _ grad()` 手动设置为零的。需要归零是因为梯度是按照 [Backprop](#backprop) 部分的说明累加的。"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
