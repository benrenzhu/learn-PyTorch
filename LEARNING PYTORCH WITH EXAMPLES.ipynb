{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PyTorch提供两个重要特性：\n",
    "- n维张量\n",
    "- 自动求导\n",
    "我们将使用一个利用三次多项式来拟合y=sin(x) 的问题作为例子。这个网络有4个参数，我们通过梯度下降法来最小化网络的输出和真实输出之间的欧氏距离，以此拟合随机输入的数据。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "一、张量\n",
    "- 热身：Numpy\n",
    "在介绍 PyTorch 之前，我们先使用 Numpy 来实现一个网络。\n",
    "Numpy 提供了一个 n 维数组对象和许多操作这些数组的函数。Numpy 是一个用于科学计算的通用框架; 它不包含任何关于计算图、深度学习或梯度的内容。\n",
    "但是，我们可以很容易地使用 numpy 将一个三阶多项式拟合成正弦函数，方法是通过 numpy 的一系列 operation 手动实现网络的向前和向后传递:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "999 31.18343170869768\n",
      "1999 0.7115274728590752\n",
      "2999 0.018084200831835002\n",
      "3999 0.0005004161569510489\n",
      "4999 1.465502851865267e-05\n",
      "5999 4.438764665261046e-07\n",
      "6999 1.3696837177327227e-08\n",
      "7999 4.268448416317019e-10\n",
      "8999 1.337055270718228e-11\n",
      "9999 4.1992704514163467e-13\n",
      "Result: y = 0.9999999783399893 + 1.99999999894079 x + 3.0000000037367123 x^2 + 4.0000000001506635 x^3\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import math\n",
    "\n",
    "#创建随机的输入和输出数据\n",
    "x = np.linspace(-math.pi, math.pi, 2000)  #在指定的间隔内返回均匀间隔的数字。[起始，结束，数量]\n",
    "# y = np.sin(x)\n",
    "y = 1 + 2 * x + 3 * x ** 2 + 4 * x ** 3\n",
    "\n",
    "#随机生成权重\n",
    "a = np.random.randn()  #randn函数返回一个或一组样本，具有标准正态分布。\n",
    "b = np.random.randn()\n",
    "c = np.random.randn()\n",
    "d = np.random.randn()\n",
    "\n",
    "leanring_rate = 1e-6\n",
    "for t in range(10000):\n",
    "    #向前传播：计算预测的 y\n",
    "    #y = a + bx + cx^2 + dx^3\n",
    "    y_pred = a + b * x + c * x ** 2 + d * x ** 3\n",
    "    \n",
    "    #计算并打印 loss\n",
    "    loss = np.square(y_pred - y).sum()  #求平方和\n",
    "    if t % 1000 == 999:\n",
    "        print(t, loss)\n",
    "        \n",
    "    #反向传播来计算 a,b,c,d 关于 loss 的梯度\n",
    "    grad_y_pred = 2.0 * (y_pred - y)\n",
    "    grad_a = grad_y_pred.sum()\n",
    "    grad_b = (grad_y_pred * x).sum()\n",
    "    grad_c = (grad_y_pred * x ** 2).sum()\n",
    "    grad_d = (grad_y_pred * x ** 3).sum()\n",
    "    \n",
    "    #更新权重\n",
    "    a -= leanring_rate * grad_a\n",
    "    b -= leanring_rate * grad_b\n",
    "    c -= leanring_rate * grad_c\n",
    "    d -= leanring_rate * grad_d\n",
    "    \n",
    "print(f'Result: y = {a} + {b} x + {c} x^2 + {d} x^3')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- PyTorch：张量\n",
    "Numpy 框架很棒，但不能利用GPU加速。\n",
    "PyTorch 张量在概念上与 Numpy 数组类似：张量也是一个 n 维数组，PyTorch 提供了许多函数来操作这些张量。在幕后，张量可以跟踪计算图和梯度。\n",
    "跟 Numpy 不同的是，PyTorch 的张量可以用GPU来加速计算，只需指定设备即可。\n",
    "在这里，我们用 PyTorch 张量通过三阶多项式拟合正弦函数。同上一部分一样，我们来手动实现网络中的向前和向后传递。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "999 144.64390563964844\n",
      "1999 2.4527010917663574\n",
      "2999 0.04366637021303177\n",
      "3999 0.0008387069683521986\n",
      "4999 1.9538689230103046e-05\n",
      "5999 2.5563788312865654e-06\n",
      "6999 2.5563788312865654e-06\n",
      "7999 2.5563788312865654e-06\n",
      "8999 2.5563788312865654e-06\n",
      "9999 2.5563788312865654e-06\n",
      "Result: y = 0.9999729990959167 + 1.999958872795105 x + 3.0000059604644775 x^2 + 4.000006198883057 x^3\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import math\n",
    "\n",
    "\n",
    "dtype = torch.float\n",
    "# device = torch.device(\"cpu\")\n",
    "defvice = torch.device(\"cuda\")\n",
    "\n",
    "# 创建随机输入和输出数据\n",
    "x = torch.linspace(-math.pi, math.pi, 2000, device=device, dtype=dtype)\n",
    "# y = torch.sin(x)\n",
    "y = 1 + 2 * x + 3 * x ** 2 + 4 * x ** 3\n",
    "\n",
    "# 随机初始化权重\n",
    "a = torch.randn((), device=device, dtype=dtype)\n",
    "b = torch.randn((), device=device, dtype=dtype)\n",
    "c = torch.randn((), device=device, dtype=dtype)\n",
    "d = torch.randn((), device=device, dtype=dtype)\n",
    "\n",
    "learning_rate = 1e-6\n",
    "for t in range(10000):\n",
    "    # 向前传播：预测 y\n",
    "    y_pred = a + b * x + c * x ** 2 + d * x **3\n",
    "    \n",
    "    # 计算并打印 loss\n",
    "    loss = (y_pred - y).pow(2).sum().item()\n",
    "    if t % 1000 == 999:\n",
    "        print(t, loss)\n",
    "    # 向后传播并计算 a，b，c，d关于 loss 的梯度\n",
    "    grad_y_pred = 2.0 * (y_pred - y)\n",
    "    grad_a = grad_y_pred.sum()\n",
    "    grad_b = (grad_y_pred * x).sum()\n",
    "    grad_c = (grad_y_pred * x ** 2).sum()\n",
    "    grad_d = (grad_y_pred * x ** 3).sum()\n",
    "    \n",
    "    # 用梯度下降更新权重\n",
    "    a -= learning_rate * grad_a\n",
    "    b -= learning_rate * grad_b\n",
    "    c -= learning_rate * grad_c\n",
    "    d -= learning_rate * grad_d\n",
    "    \n",
    "print(f'Result: y = {a.item()} + {b.item()} x + {c.item()} x^2 + {d.item()} x^3')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "二、自动求导\n",
    "- PyTorch：张量和自动求导\n",
    "上面的例子很简单，手动实现很容易，大型网络就不方便手动实现了。\n",
    "但有了 PyTorch，我们可以使用 PyTorch 中的 autograd 包，来自动计算神经网络中的反向传递。\n",
    "使用 autograd 时，网络的向前传递将定义一个计算图; 图中的节点将是张量，边将是从输入张量产生输出张量的函数。\n",
    "反向传播通过这个图来计算梯度。  \n",
    "\n",
    "听起来复杂实践起来简单。如果 x 是一个 `x.require _ grad = True` 的张量，那么x 对某个标量值的梯度则可以通过 `x.grad` 获得。  \n",
    "\n",
    "现在我们通过 PyTorch 张量的自动求导来将三次多项式拟合正弦函数。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "99 13642.4794921875\n",
      "199 9031.787109375\n",
      "299 5979.84912109375\n",
      "399 3959.5361328125\n",
      "499 2622.03955078125\n",
      "599 1736.507568359375\n",
      "699 1150.166748046875\n",
      "799 761.8931274414062\n",
      "899 504.7524719238281\n",
      "999 334.44122314453125\n",
      "1099 221.62559509277344\n",
      "1199 146.88694763183594\n",
      "1299 97.36748504638672\n",
      "1399 64.55303192138672\n",
      "1499 42.80545425415039\n",
      "1599 28.389713287353516\n",
      "1699 18.83257484436035\n",
      "1799 12.495615005493164\n",
      "1899 8.292737007141113\n",
      "1999 5.504903793334961\n",
      "Result: y = 0.9809680581092834 + 1.9300259351730347 x + 3.003283739089966 x^2 + 4.009953498840332 x^3\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import math\n",
    "\n",
    "dtype = torch.float\n",
    "device = torch.device(\"cpu\")\n",
    "# defvice = torch.device(\"cuda:0\")\n",
    "\n",
    "# 创建随机输入和输出数据\n",
    "# 张量的 requires_grad 默认为 False\n",
    "x = torch.linspace(-math.pi, math.pi, 2000, device=device, dtype=dtype)\n",
    "# y = torch.sin(x)\n",
    "y = 1 + 2 * x + 3 * x ** 2 + 4 * x ** 3\n",
    "\n",
    "# 创建四个权重张量 a, b, c, d: y = a + b x + c x^2 + d x^3\n",
    "# requires_grad=True 意味着我们要计算其梯度\n",
    "a = torch.randn((), device=device, dtype=dtype, requires_grad=True)\n",
    "b = torch.randn((), device=device, dtype=dtype, requires_grad=True)\n",
    "c = torch.randn((), device=device, dtype=dtype, requires_grad=True)\n",
    "d = torch.randn((), device=device, dtype=dtype, requires_grad=True)\n",
    "\n",
    "learning_rate = 1e-6\n",
    "for t in range(2000):\n",
    "    # 向前传播：通过操作张量来预测 y\n",
    "    y_pred = a + b * x + c * x ** 2 + d * x ** 3\n",
    "\n",
    "    # 通过操作张量计算并打印 loss\n",
    "    # 现在 loss 是一个形状为 （1，）的张量\n",
    "    # loss.item() 用来获取 loss 的标量值\n",
    "    loss = (y_pred - y).pow(2).sum()\n",
    "    if t % 100 == 99:\n",
    "        print(t, loss.item())\n",
    "        \n",
    "    # 通过自动求导来计算反向传播的梯度的过程，PyTorch 会计算所有 requires_grad=True 的标量\n",
    "    # 自动求导后，a.grad，b.grad等等会作为张量各自保存着 a，b 对应的关于 loss 的梯度\n",
    "    loss.backward()\n",
    "\n",
    "    # 手动更新权重（a，b，c，d）的时候，不需要自动求导的计算图追踪他们的计算过程\n",
    "    with torch.no_grad():\n",
    "        a -= learning_rate * a.grad\n",
    "        b -= learning_rate * b.grad\n",
    "        c -= learning_rate * c.grad\n",
    "        d -= learning_rate * d.grad\n",
    "\n",
    "        # 权重更新后手动清零梯度\n",
    "        a.grad = None\n",
    "        b.grad = None\n",
    "        c.grad = None\n",
    "        d.grad = None\n",
    "            \n",
    "print(f'Result: y = {a.item()} + {b.item()} x + {c.item()} x^2 + {d.item()} x^3')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- PyTorch：定义新的自动求导公式\n",
    "\n",
    "实际上，每个基本的 autograd 操作符实际上是两个对 Tensors 进行操作的函数。**正向函数**从输入张量计算输出张量。**反向函数**接收输出张量相对于某个标量值的梯度，并计算输入张量相对于同一标量值的梯度。\n",
    "\n",
    "\n",
    "在 PyTorch 中，我们可以通过定义 `torch.autograd.Function` 的子类来定义自己的 autograd 操作符,实现前向后向传播。然后，我们可以通过构造一个实例并像调用函数一样调用它，传递包含输入数据的 Tensors 来使用新的 autograd 操作符。\n",
    "\n",
    "在下面的例子中，我们把模型定义为 $y=a+bP_3(c+dx)$，其中 $P_3(x)=\\frac{1}{2}5x^3-3x$ 是三次勒让德多项式。\n",
    "接下来自定义 autograd 函数来计算 $P_3$ 的前向和后向传播，并用其实现我们夫人模型。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "99 16.171911239624023\n",
      "199 13.80992317199707\n",
      "299 12.206977844238281\n",
      "399 11.118906021118164\n",
      "499 10.380213737487793\n",
      "599 9.878663063049316\n",
      "699 9.538034439086914\n",
      "799 9.306766510009766\n",
      "899 9.149720191955566\n",
      "999 9.043050765991211\n",
      "1099 8.970601081848145\n",
      "1199 8.92138671875\n",
      "1299 8.88795280456543\n",
      "1399 8.865248680114746\n",
      "1499 8.84982681274414\n",
      "1599 8.83935546875\n",
      "1699 8.832240104675293\n",
      "1799 8.82740592956543\n",
      "1899 8.824119567871094\n",
      "1999 8.821889877319336\n",
      "Result: y = -7.463839324373112e-09 + -2.2291290760040283 * P3(2.462566817129641e-09 + 0.2556264400482178 x)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import math\n",
    "\n",
    "class LegendrePolynomial3(torch.autograd.Function):\n",
    "    @staticmethod\n",
    "    def forward(ctx, input):\n",
    "        \"\"\"\n",
    "        在向前传播当中，我们接收一个 Tensor 作为输入，返回一个 Tensor 作为输出。\n",
    "        ctx 是一个用来隐藏反向传播信息的上下文对象。\n",
    "        可以使用 ctx.save_for_backward 方法缓存任意对象，以便在反向传播中使用。\n",
    "        \"\"\"\n",
    "        ctx.save_for_backward(input)\n",
    "        return 0.5 * (5 * input ** 3 - 3 * input)\n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output):\n",
    "        \"\"\"\n",
    "        在反向传播中，我们接收一个包含关于 loss 的损失梯度的张量，我们需要计算 loss 关于 input 的梯度。\n",
    "        \"\"\"\n",
    "        input, = ctx.saved_tensors\n",
    "        return grad_output * 1.5 * (5 * input ** 2 - 1)\n",
    "    \n",
    "dtype = torch.float\n",
    "device = torch.device(\"cpu\")\n",
    "# device = torch.device(\"cuda:0\")\n",
    "\n",
    "# 创建随机输入和输出数据\n",
    "x = torch.linspace(-math.pi, math.pi, 2000, device=device, dtype=dtype)\n",
    "y = torch.sin(x)\n",
    "\n",
    "a = torch.full((), 0.0, device=device, dtype=dtype, requires_grad=True)\n",
    "b = torch.full((), -1.0, device=device, dtype=dtype, requires_grad=True)\n",
    "c = torch.full((), 0.0, device=device, dtype=dtype, requires_grad=True)\n",
    "d = torch.full((), 0.3, device=device, dtype=dtype, requires_grad=True)\n",
    "\n",
    "learning_rate = 5e-6\n",
    "for t in range(2000):\n",
    "    # 用 Function.apply 方法来使用我们的函数，命名为 P3\n",
    "    P3 = LegendrePolynomial3.apply\n",
    "    \n",
    "    # 我们用自定义的自动求导操作来计算P3\n",
    "    y_pred = a + b * P3(c + d * x)\n",
    "    \n",
    "    loss = (y_pred - y).pow(2).sum()\n",
    "    if t % 100 == 99:\n",
    "        print(t, loss.item())\n",
    "        \n",
    "    loss.backward()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        a -= learning_rate * a.grad\n",
    "        b -= learning_rate * b.grad\n",
    "        c -= learning_rate * c.grad\n",
    "        d -= learning_rate * d.grad\n",
    "\n",
    "        a.grad = None\n",
    "        b.grad = None\n",
    "        c.grad = None\n",
    "        d.grad = None\n",
    "\n",
    "print(f'Result: y = {a.item()} + {b.item()} * P3({c.item()} + {d.item()} x)')\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "三、nn 模块\n",
    "\n",
    "- PyTorch：nn\n",
    "\n",
    "计算图和自动梯度是定义复杂运算符和自动求导的一个非常强大的范例，但是对于大型神经网络来说，光是自动求导还是有点不太够用。\n",
    "\n",
    "在构建神经网络时，我们经常考虑将计算分层，某几层有可学习的参数，可以在学习过程中进行优化。\n",
    "\n",
    "在 PyTorch 中，`nn` 包定义了一组模块，它们大致等价于神经网络层。模块的输入和输出虽然都是张量，但是也可以保存内部状态，比如包含了可学习参数的张量。`nn` 包还定义了一组常用于训练神经网络的损失函数。\n",
    "\n",
    "在下面的例子中，我们使用 `nn` 来实现多项式网络模型："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "99 2744.318603515625\n",
      "199 1848.897216796875\n",
      "299 1246.8074951171875\n",
      "399 841.5919799804688\n",
      "499 568.632568359375\n",
      "599 384.59356689453125\n",
      "699 260.3876953125\n",
      "799 176.4799346923828\n",
      "899 119.73834991455078\n",
      "999 81.33010864257812\n",
      "1099 55.302528381347656\n",
      "1199 37.64710235595703\n",
      "1299 25.656755447387695\n",
      "1399 17.50545883178711\n",
      "1499 11.95757007598877\n",
      "1599 8.177275657653809\n",
      "1699 5.598736763000488\n",
      "1799 3.8375887870788574\n",
      "1899 2.633410930633545\n",
      "1999 1.8091998100280762\n",
      "Result: y = 0.9659658074378967 + 1.9729329347610474 x + 3.005871295928955 x^2 + 4.003849983215332 x^3\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import math\n",
    "\n",
    "x = torch.linspace(-math.pi, math.pi, 2000, device=device, dtype=dtype)\n",
    "y = torch.sin(x)\n",
    "\n",
    "# 在这个例子中，输出：y 是（x, x^2, x^3）的线性方程，所以我们把它看作一个线性网络\n",
    "# 准备张量（x, x^2, x^3）\n",
    "p = torch.tensor([1, 2, 3])\n",
    "xx = x.unsqueeze(-1).pow(p)\n",
    "\n",
    "# 上面代码中，x.unsqueeze(-1) 的形状是（2000，1），p的形状是（3，）\n",
    "# 这种情况下，广播语义可以获得一个（2000，3）尺寸的张量\n",
    "# 使用 nn 包可以将我们的模型定义为一系列的层，nn.Sequential 包含其他模块，按顺序执行得到 output\n",
    "# 线性模块使用线性函数计算 output ，并保存内部张量的权重和偏差。\n",
    "# Flatten 层将线性层的输出压平为一维张量来匹配 y 的形状\n",
    "\n",
    "model = torch.nn.Sequential(\n",
    "    torch.nn.Linear(3, 1),\n",
    "    torch.nn.Flatten(0, 1)\n",
    ")\n",
    "\n",
    "# nn 还包含常用损失函数的定义，在这个例子中，我们使用均方误差（MSE）作为损失函数\n",
    "loss_fn = torch.nn.MSELoss(reduction='sum')\n",
    "\n",
    "learning_rate = 1e-6\n",
    "for t in range(2000):\n",
    "    y_pred = model(xx)\n",
    "    loss = loss_fn(y_pred, y)\n",
    "    if t % 100 == 99:\n",
    "        print(t, loss.item())\n",
    "        \n",
    "    # 在运行向后传播之前将梯度调零\n",
    "    model.zero_grad()\n",
    "    \n",
    "    # 反向传播: 计算模型所有可学习参数（requires_grad=True）的损失梯度\n",
    "    loss.backward()\n",
    "    \n",
    "    # 使用梯度下降法更新权重。每个参数都是一个张量，所以我们可以像往常一样访问它的梯度\n",
    "    with torch.no_grad():\n",
    "        for param in model.parameters():\n",
    "            param -= leanring_rate * param.grad\n",
    "    \n",
    "    # 你可以像访问列表的第一项一样访问模型的第一层\n",
    "    linear_layer = model[0]\n",
    "    # 对于线性层，其参数以“权重”和“偏置”的形式存储\n",
    "print(f'Result: y = {linear_layer.bias.item()} + {linear_layer.weight[:, 0].item()} x + {linear_layer.weight[:, 1].item()} x^2 + {linear_layer.weight[:, 2].item()} x^3')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- PyTorch：优化\n",
    "目前为止，我们已经通过 `torch.no_grad()` 手动更新了模型的权重。对于 *像随机梯度下降* 这样的简单优化算法来说，还不算难，但是在实践中，我们经常使用更复杂的优化器来训练神经网络，比如 AdaGrad，RMSProp，Adam 等。\n",
    "\n",
    "PyTorch 中的 `optim` 包抽象了优化算法的思想，并提供了常用优化算法的实现。\n",
    "\n",
    "在下面的例子中，我们将像以前一样使用 nn 包来定义我们的模型，但是将使用 optim 包提供的 RMSprop 算法来优化模型:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "99 6221316.5\n",
      "199 5868048.0\n",
      "299 5555701.5\n",
      "399 5261233.5\n",
      "499 4978179.5\n",
      "599 4704368.5\n",
      "699 4439034.0\n",
      "799 4181887.25\n",
      "899 3932820.25\n",
      "999 3691787.75\n",
      "1099 3458767.5\n",
      "1199 3233744.0\n",
      "1299 3016703.75\n",
      "1399 2807635.0\n",
      "1499 2606526.0\n",
      "1599 2413360.5\n",
      "1699 2228125.5\n",
      "1799 2050801.875\n",
      "1899 1881374.0\n",
      "1999 1719820.125\n",
      "Result: y = 2.5622239112854004 + 1.9125585556030273 x + 1.5245548486709595 x^2 + 1.5586963891983032 x^3\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import math\n",
    "\n",
    "x = torch.linspace(-math.pi, math.pi, 2000)\n",
    "# y = torch.sin(x)\n",
    "y = 1 + 2 * x + 3 * x ** 2 + 4 * x ** 3\n",
    "\n",
    "# 准备张量（x, x^2, x^3）\n",
    "p = torch.tensor([1, 2, 3])\n",
    "xx = x.unsqueeze(-1).pow(p)\n",
    "\n",
    "# 使用 nn 来定义模型和损失函数\n",
    "model = torch.nn.Sequential(\n",
    "    torch.nn.Linear(3, 1),\n",
    "    torch.nn.Flatten(0, 1)\n",
    ")\n",
    "loss_fn = torch.nn.MSELoss(reduction='sum')\n",
    "\n",
    "# 使用 optim 定义一个优化器来为我们更新模型的权重\n",
    "# 这里我们将使用 RMSprop\n",
    "# optim 包含许多其他优化算法\n",
    "# RMSprop 构造函数的第一个参数用来告诉优化器它应该更新哪些张量\n",
    "learning_rate = 1e-3\n",
    "optimizer = torch.optim.RMSprop(model.parameters(), lr=learning_rate)\n",
    "\n",
    "for t in range(2000):\n",
    "    y_pred = model(xx)\n",
    "    loss = loss_fn(y_pred, y)\n",
    "    if t % 100 == 99:\n",
    "        print(t, loss.item())\n",
    "        \n",
    "    # 在向后传递之前，使用优化器对象将它要更新的变量的所有的梯度归零(这些变量是模型的可学习的权重)\n",
    "    # 这是因为在默认情况下，当调用 backward（）时，梯度是在缓冲区累加的(非覆盖)\n",
    "    # 查看 torch.autograd.backwards 文档可以了解更多细节\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    # 反向传播: 计算 loss 关于模型参数的梯度\n",
    "    loss.backward()\n",
    "    \n",
    "    # 在优化器上调用 step() 函数来更新其参数\n",
    "    optimizer.step()\n",
    "    \n",
    "linear_layer = model[0]\n",
    "print(f'Result: y = {linear_layer.bias.item()} + {linear_layer.weight[:, 0].item()} x + {linear_layer.weight[:, 1].item()} x^2 + {linear_layer.weight[:, 2].item()} x^3')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- PyTorch：自定义 nn 模块\n",
    "\n",
    "有时候需要更复杂的模型，可以通过子类化`nn.Module`和一个`forward`来定义自己的模块。\n",
    "在接下来的例子中，我们将三阶多项式作为一个自定义的 Module 子类来实现:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "99 2486.525634765625\n",
      "199 1681.998046875\n",
      "299 1139.831787109375\n",
      "399 774.1256713867188\n",
      "499 527.2081298828125\n",
      "599 360.3292236328125\n",
      "699 247.4293670654297\n",
      "799 170.96926879882812\n",
      "899 119.13241577148438\n",
      "999 83.95153045654297\n",
      "1099 60.04827117919922\n",
      "1199 43.78956604003906\n",
      "1299 32.71812438964844\n",
      "1399 25.170413970947266\n",
      "1499 20.019065856933594\n",
      "1599 16.499235153198242\n",
      "1699 14.091410636901855\n",
      "1799 12.4423828125\n",
      "1899 11.311720848083496\n",
      "1999 10.535612106323242\n",
      "Result: y = -0.03431374952197075 + 0.83162522315979 x + 0.005919694900512695 x^2 + -0.08975791186094284 x^3\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import math\n",
    "\n",
    "class Polynomial3(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        \"\"\"\n",
    "        在构造函数中，我们实例化四个参数，并将它们作为成员参数分配\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.a = torch.nn.Parameter(torch.randn(()))\n",
    "        self.b = torch.nn.Parameter(torch.randn(()))\n",
    "        self.c = torch.nn.Parameter(torch.randn(()))\n",
    "        self.d = torch.nn.Parameter(torch.randn(()))\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        在前向函数中，我们接受一个输入数据的张量，并且必须返回一个用来输出数据的张量。\n",
    "        我们可以使用构造函数中定义的模块，也可以使用 Tensors 上的任意操作符。\n",
    "        \"\"\"\n",
    "        return self.a + self.b * x + self.c * x ** 2 + self.d * x ** 3\n",
    "    \n",
    "    def string(self):\n",
    "        \"\"\"\n",
    "        就像 Python 中的任何类一样，您也可以在 PyTorch 模块上自定义方法\n",
    "        \"\"\"\n",
    "        return f'y = {self.a.item()} + {self.b.item()} x + {self.c.item()} x^2 + {self.d.item()} x^3'\n",
    "    \n",
    "x = torch.linspace(-math.pi, math.pi, 2000)\n",
    "y = torch.sin(x)\n",
    "\n",
    "# 通过实例化上面定义的类来构造我们的模型\n",
    "model = Polynomial3()\n",
    "\n",
    "# 构造我们的损失函数和优化器\n",
    "# SGD 构造函数中 model.parameters ()指的是 nn.Linear 模块中的可学习参数\n",
    "criterion = torch.nn.MSELoss(reduction='sum')\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=1e-6)\n",
    "for t in range(2000):\n",
    "    y_pred = model(x)\n",
    "    \n",
    "    loss = criterion(y_pred, y)\n",
    "    if t % 100 == 99:\n",
    "        print(t, loss.item())\n",
    "        \n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "print(f'Result: {model.string()}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- PyTorch：控制流+权重共享\n",
    "\n",
    "作为动态图和权重分配的一个例子，我们要实现一个非常奇怪的模型: 一个三阶-五阶多项式，每次向前传递选择一个介于3到5之间的随机数作为阶数，重复使用相同的权重计算四阶和五阶\n",
    "\n",
    "对于这个模型，我们可以使用普通的 Python 流控制来实现循环，而且我们可以通过在定义向前传递时多次重用同一个参数来实现权重共享。\n",
    "\n",
    "我们可以很容易地将这个模型作为一个 Module 子类来实现:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1999 3313.55224609375\n",
      "3999 1525.671630859375\n",
      "5999 700.5814208984375\n",
      "7999 346.0111389160156\n",
      "9999 158.0843963623047\n",
      "11999 78.30085754394531\n",
      "13999 41.366275787353516\n",
      "15999 24.08107566833496\n",
      "17999 15.956063270568848\n",
      "19999 12.25631332397461\n",
      "21999 10.454581260681152\n",
      "23999 9.577637672424316\n",
      "25999 8.980056762695312\n",
      "27999 9.027199745178223\n",
      "29999 8.65000057220459\n",
      "Result: y = 0.009211992844939232 + 0.853203535079956 x + -0.002156247617676854 x^2 + -0.09316753596067429 x^3 + 0.00012350086763035506 x^4 ? + 0.00012350086763035506 x^5 ?\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import torch\n",
    "import math\n",
    "\n",
    "\n",
    "class DynamicNet(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        \"\"\"\n",
    "        在构造函数中，我们实例化五个参数，并将它们分配为成员x\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.a = torch.nn.Parameter(torch.randn(()))\n",
    "        self.b = torch.nn.Parameter(torch.randn(()))\n",
    "        self.c = torch.nn.Parameter(torch.randn(()))\n",
    "        self.d = torch.nn.Parameter(torch.randn(()))\n",
    "        self.e = torch.nn.Parameter(torch.randn(()))\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        对于模型的前向传递，我们随机选择4,5和重用参数 e 来计算这些阶的贡献。\n",
    "        \n",
    "        由于每次向前传递都构建一个动态计算图，因此在定义模型向前传递时，我们可以使用常规的 Python 控制流运算符，如循环或条件语句。\n",
    "        \n",
    "        这里我们还看到，在定义计算图时，多次重用相同的参数是非常安全的。\n",
    "        \"\"\"\n",
    "        y = self.a + self.b * x + self.c * x ** 2 + self.d * x ** 3\n",
    "        for exp in range(4, random.randint(4, 6)):\n",
    "            y = y + self.e * x ** exp\n",
    "        return y\n",
    "    \n",
    "    def string(self):\n",
    "        \"\"\"\n",
    "        就像 Python 中的任何类一样，您也可以在 PyTorch 模块上自定义方法\n",
    "        \"\"\"\n",
    "        return f'y = {self.a.item()} + {self.b.item()} x + {self.c.item()} x^2 + {self.d.item()} x^3 + {self.e.item()} x^4 ? + {self.e.item()} x^5 ?'\n",
    "    \n",
    "x = torch.linspace(-math.pi, math.pi, 2000)\n",
    "y = torch.sin(x)\n",
    "\n",
    "# 通过实例化上面定义的类来构造我们的模型\n",
    "model = DynamicNet()\n",
    "\n",
    "criterion = torch.nn.MSELoss(reduction='sum')\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=1e-8, momentum=0.9)\n",
    "for t in range(30000):\n",
    "    y_pred = model(x)\n",
    "    loss = criterion(y_pred, y)\n",
    "    if t % 2000 == 1999:\n",
    "        print(t, loss.item())\n",
    "    \n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "print(f'Result: {model.string()}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
